
@misc{hamalainen_video_2022,
	title = {Video {Games} as a {Corpus}: {Sentiment} {Analysis} using {Fallout} {New} {Vegas} {Dialog}},
	shorttitle = {Video {Games} as a {Corpus}},
	url = {https://arxiv.org/abs/2212.02168v1},
	abstract = {We present a method for extracting a multilingual sentiment annotated dialog data set from Fallout New Vegas. The game developers have preannotated every line of dialog in the game in one of the 8 different sentiments: {\textbackslash}textit\{anger, disgust, fear, happy, neutral, pained, sad \} and {\textbackslash}textit\{surprised\}. The game has been translated into English, Spanish, German, French and Italian. We conduct experiments on multilingual, multilabel sentiment analysis on the extracted data set using multilingual BERT, XLMRoBERTa and language specific BERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for most of the languages, also language specific models were slightly better than multilingual BERT for most of the languages. The best overall accuracy was 54 percent and it was achieved by using multilingual BERT on Spanish data. The extracted data set presents a challenging task for sentiment analysis. We have released the data, including the testing and training splits, openly on Zenodo. The data set has been shuffled for copyright reasons.},
	language = {en},
	urldate = {2023-04-02},
	journal = {arXiv.org},
	author = {Hämäläinen, Mika and Alnajjar, Khalid and Poibeau, Thierry},
	month = dec,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\CQHCHJQG\\Hämäläinen et al. - 2022 - Video Games as a Corpus Sentiment Analysis using .pdf:application/pdf},
}

@inproceedings{maghilnan_sentiment_2017,
	title = {Sentiment analysis on speaker specific speech data},
	doi = {10.1109/I2C2.2017.8321795},
	abstract = {Sentiment analysis has evolved over past few decades, most of the work in it revolved around textual sentiment analysis with text mining techniques. But audio sentiment analysis is still in a nascent stage in the research community. In this proposed research, we perform sentiment analysis on speaker discriminated speech transcripts to detect the emotions of the individual speakers involved in the conversation. We analyzed different techniques to perform speaker discrimination and sentiment analysis to find efficient algorithms to perform this task.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} ({I2C2})},
	author = {Maghilnan, S and Kumar, M Rajesh},
	month = jun,
	year = {2017},
	keywords = {DTW, Feature extraction, Mel frequency cepstral coefficient, MFCC, Sentiment analysis, Sentiment Analysis, Speaker recognition, Speaker Recognition, Speech, Speech recognition, Speech Recognition, Time series analysis},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kiera\\Zotero\\storage\\PS7YL2EV\\8321795.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\MWXBK6ZM\\Maghilnan and Kumar - 2017 - Sentiment analysis on speaker specific speech data.pdf:application/pdf},
}

@misc{noauthor_mutagen_2023,
	title = {Mutagen},
	copyright = {GPL-3.0},
	url = {https://github.com/Mutagen-Modding/Mutagen},
	abstract = {A .NET library for analyzing, creating, and manipulating Bethesda mods},
	urldate = {2023-04-03},
	publisher = {Mutagen-Modding},
	month = mar,
	year = {2023},
	note = {original-date: 2017-07-14T01:58:38Z},
}

@article{murugaiyan_aspect-based_2023,
	title = {Aspect-{Based} {Sentiment} {Analysis} of {Customer} {Speech} {Data} {Using} {Deep} {Convolutional} {Neural} {Network} and {BiLSTM}},
	issn = {1866-9964},
	url = {https://doi.org/10.1007/s12559-023-10127-6},
	doi = {10.1007/s12559-023-10127-6},
	abstract = {The process of detecting sentiments of particular context from human speech emotions is naturally in-built for humans unlike computers, where it is not possible to process human emotions by a machine for predicting sentiments of a particular context. Though machines can easily understand the content-based information, accessing the real emotion behind it is difficult. Aspect-based sentiment analysis based on speech emotion recognition framework can bridge the gap between these problems. The proposed model helps people with autism spectrum disorder (ASD) to understand other’s sentiments expressed through speech data about the recently purchased product based on various aspects of the product. It is a framework through which different sound discourse documents are characterized into various feelings like happy, sad, anger, and neutral and label the sound with aspect-wise sentiment polarity. This study proposed a hybrid model using deep convolutional neural networks (DCNN) for speech emotion recognition, bidirectional long short term memory (BiLSTM) for speech aspect recognition, and rule-based classifier for aspect-wise sentiment classification. In the existing work, sentiment analysis was carried out on speech data, but aspect-based sentiment analysis on speech data was not carried out successfully. The proposed model extracted standard Mel frequency cepstral coefficient (MFCC) features from customer speech data about product review and generated aspect-wise sentiment label. Enhanced cat swarm optimization (ECSO) algorithm was used for selection features from the extracted feature in the proposed model that improved the overall sentiment classification accuracy. The proposed hybrid framework obtained promising results on sentiment classification accuracy of 93.28\%, 91.45\%, 92.12\%, and 90.45\% on four benchmark datasets. The proposed hybrid framework sentiment classification accuracy on these benchmark datasets were compared with other CNN variants and shown better performance. Sentiment classification accuracy of the proposed model with state-of-art methods on the four benchmark datasets was compared and shown better performance. Aspect classification accuracy of the proposed with state-of-art methods on the benchmark datasets was compared and shown better performance. The developed hybrid model using DCNN, BiLSTM, and rule-based classifier outperformed the state-of-art models for aspect-based sentiment analysis by incorporating ECSO algorithm in feature selection process. The proposed model will help to perform aspect-based sentiment analysis on all domains with specified aspect corpus.},
	language = {en},
	urldate = {2023-04-14},
	journal = {Cognitive Computation},
	author = {Murugaiyan, Sivakumar and Uyyala, Srinivasulu Reddy},
	month = mar,
	year = {2023},
	keywords = {Aspect based sentiment analysis, Convolutional neural network, Customer speech review, Deep learning, Machine learning, Speech emotion recognition},
	file = {Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\3Y3AY86D\\Murugaiyan and Uyyala - 2023 - Aspect-Based Sentiment Analysis of Customer Speech.pdf:application/pdf},
}

@article{bhaskar_hybrid_2015,
	series = {Proceedings of the {International} {Conference} on {Information} and {Communication} {Technologies}, {ICICT} 2014, 3-5 {December} 2014 at {Bolgatty} {Palace} \& {Island} {Resort}, {Kochi}, {India}},
	title = {Hybrid {Approach} for {Emotion} {Classification} of {Audio} {Conversation} {Based} on {Text} and {Speech} {Mining}},
	volume = {46},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050915001763},
	doi = {10.1016/j.procs.2015.02.112},
	abstract = {One of the greatest challenges in speech technology is estimating the speaker's emotion. Most of the existing approaches concentrate either on audio or text features. In this work, we propose a novel approach for emotion classification of audio conversation based on both speech and text. The novelty in this approach is in the choice of features and the generation of a single feature vector for classification. Our main intention is to increase the accuracy of emotion classification of speech by considering both audio and text features. In this work we use standard methods such as Natural Language Processing, Support Vector Machines, WordNet Affect and SentiWordNet. The dataset for this work have been taken from Semval -2007 and eNTERFACE’05 EMOTION Database.},
	language = {en},
	urldate = {2023-04-17},
	journal = {Procedia Computer Science},
	author = {Bhaskar, Jasmine and Sruthi, K. and Nedungadi, Prema},
	month = jan,
	year = {2015},
	keywords = {Emotion classification, hybrid approach, speech mining, text mining},
	pages = {635--643},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\EJP5QRCZ\\Bhaskar et al. - 2015 - Hybrid Approach for Emotion Classification of Audi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\kiera\\Zotero\\storage\\BG8KFV7Q\\S1877050915001763.html:text/html},
}

@misc{noclip_-_video_game_documentaries_music_2018,
	title = {The {Music} \& {Sound} of {Bethesda} {RPGs} ({Skyrim}, {Oblivion}, {Fallout})},
	url = {https://www.youtube.com/watch?v=3193ZsxChSc},
	abstract = {Mark Lampert details the work that goes into designing the soundscape of a Fallout and Elder Scrolls games and tells us about the work of composers Jeremy Soule \& Inon Zur.

Noclip's work is 100\% crowdfunded. Consider supporting us on Patreon: https://www.patreon.com/noclip

Check out our new website here: https://www.noclip.video
Buy merch here: https://store.noclip.video

Music Used:
Dusk at the Market - Oblivion OST
   • The Elder Scrolls...  
Explore 2 - Fallout 3 OST
   • Video  
Dragonborne - Skyrim OST
   • TES V Skyrim - DR...  
Dragon's Reach - Skyrim OST
   • TES V Skyrim Soun...  
The White River - Skyrim OST
   • TES V Skyrim Soun...  
No Quarter - Fallout 4 OST
   • Fallout 4 OST - N...  
Bloody Blades - Oblivion OST
   • Video  

00:00 Elder Scrolls Audio
03:26 Sounds of Fallout
06:44 Music
11:45 Credits},
	urldate = {2023-04-19},
	author = {{Noclip - Video Game Documentaries}},
	month = jul,
	year = {2018},
}

@inproceedings{stone_computer_1963,
	address = {New York, NY, USA},
	series = {{AFIPS} '63 ({Spring})},
	title = {A computer approach to content analysis: studies using the {General} {Inquirer} system},
	isbn = {978-1-4503-7880-2},
	shorttitle = {A computer approach to content analysis},
	url = {https://dl.acm.org/doi/10.1145/1461551.1461583},
	doi = {10.1145/1461551.1461583},
	abstract = {The General Inquirer is an IBM 7090 program system that was developed at Harvard in the spring of 1961 for content analysis research problems in the behavioral sciences. The first part of this paper describes this system and how it has been used. During the summer of 1962, the General Inquirer was merged with the Hunt Concept Learner to produce a method for automatic theme analysis. The second part of this paper discusses the rationale behind this development and some recent signs of its future promise.},
	urldate = {2023-04-20},
	booktitle = {Proceedings of the {May} 21-23, 1963, spring joint computer conference},
	publisher = {Association for Computing Machinery},
	author = {Stone, Philip J. and Hunt, Earl B.},
	month = may,
	year = {1963},
	pages = {241--256},
	file = {Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\PLYVVJLY\\Stone and Hunt - 1963 - A computer approach to content analysis studies u.pdf:application/pdf},
}

@article{wollmer_lstm-modeling_2013,
	series = {Affect {Analysis} {In} {Continuous} {Input}},
	title = {{LSTM}-{Modeling} of continuous emotions in an audiovisual affect recognition framework},
	volume = {31},
	issn = {0262-8856},
	url = {https://www.sciencedirect.com/science/article/pii/S0262885612000285},
	doi = {10.1016/j.imavis.2012.03.001},
	abstract = {Automatically recognizing human emotions from spontaneous and non-prototypical real-life data is currently one of the most challenging tasks in the field of affective computing. This article presents our recent advances in assessing dimensional representations of emotion, such as arousal, expectation, power, and valence, in an audiovisual human–computer interaction scenario. Building on previous studies which demonstrate that long-range context modeling tends to increase accuracies of emotion recognition, we propose a fully automatic audiovisual recognition approach based on Long Short-Term Memory (LSTM) modeling of word-level audio and video features. LSTM networks are able to incorporate knowledge about how emotions typically evolve over time so that the inferred emotion estimates are produced under consideration of an optimal amount of context. Extensive evaluations on the Audiovisual Sub-Challenge of the 2011 Audio/Visual Emotion Challenge show how acoustic, linguistic, and visual features contribute to the recognition of different affective dimensions as annotated in the SEMAINE database. We apply the same acoustic features as used in the challenge baseline system whereas visual features are computed via a novel facial movement feature extractor. Comparing our results with the recognition scores of all Audiovisual Sub-Challenge participants, we find that the proposed LSTM-based technique leads to the best average recognition performance that has been reported for this task so far.},
	language = {en},
	number = {2},
	urldate = {2023-04-22},
	journal = {Image and Vision Computing},
	author = {Wöllmer, Martin and Kaiser, Moritz and Eyben, Florian and Schuller, Björn and Rigoll, Gerhard},
	month = feb,
	year = {2013},
	keywords = {Context modeling, Emotion recognition, Facial movement features, Long Short-Term Memory},
	pages = {153--163},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\GDE23FFH\\Wöllmer et al. - 2013 - LSTM-Modeling of continuous emotions in an audiovi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\kiera\\Zotero\\storage\\W6LYTY2X\\S0262885612000285.html:text/html},
}

@inproceedings{wollmer_abandoning_2008,
	title = {Abandoning emotion classes - towards continuous emotion recognition with modelling of long-range dependencies},
	url = {https://www.isca-speech.org/archive/interspeech_2008/wollmer08_interspeech.html},
	doi = {10.21437/Interspeech.2008-192},
	abstract = {Class based emotion recognition from speech, as performed in most works up to now, entails many restrictions for practical applications. Human emotion is a continuum and an automatic emotion recognition system must be able to recognise it as such. We present a novel approach for continuous emotion recognition based on Long Short-Term Memory Recurrent Neural Networks which include modelling of long-range dependencies between observations and thus outperform techniques like Support-Vector Regression. Transferring the innovative concept of additionally modelling emotional history to the classiﬁcation of discrete levels for the emotional dimensions “valence” and “activation” we also apply Conditional Random Fields which prevail over the commonly used Support-Vector Machines. Experiments conducted on data that was recorded while humans interacted with a Sensitive Artiﬁcial Listener prove that for activation the derived classiﬁers perform as well as human annotators.},
	language = {en},
	urldate = {2023-04-22},
	booktitle = {Interspeech 2008},
	publisher = {ISCA},
	author = {Wöllmer, Martin and Eyben, Florian and Reiter, Stephan and Schuller, Björn and Cox, Cate and Douglas-Cowie, Ellen and Cowie, Roddy},
	month = sep,
	year = {2008},
	pages = {597--600},
	file = {Wöllmer et al. - 2008 - Abandoning emotion classes - towards continuous em.pdf:C\:\\Users\\kiera\\Zotero\\storage\\758FEQHD\\Wöllmer et al. - 2008 - Abandoning emotion classes - towards continuous em.pdf:application/pdf},
}

@misc{noauthor_buy_nodate,
	title = {Buy {Fallout}: {New} {Vegas} {Ultimate} {Edition} {\textbar} {Xbox}},
	url = {https://www.xbox.com/en-GB/games/store/fallout-new-vegas-ultimate-edition/9mzr11jrf7bx},
	urldate = {2023-04-22},
	file = {Buy Fallout\: New Vegas Ultimate Edition | Xbox:C\:\\Users\\kiera\\Zotero\\storage\\2BUXQLMZ\\9mzr11jrf7bx.html:text/html},
}

@misc{noauthor_elder_nodate,
	title = {The {Elder} {Scrolls} {IV}: {Oblivion} {Game} of the {Year} {Edition} ({PC}) {\textbar} {Xbox}},
	url = {https://www.xbox.com/en-gb/games/store/the-elder-scrolls-iv-oblivion-game-of-the-year-edition-pc/9p2j5d12gjl5},
	urldate = {2023-04-22},
	file = {The Elder Scrolls IV\: Oblivion Game of the Year Edition (PC) | Xbox:C\:\\Users\\kiera\\Zotero\\storage\\V9Y9CXQE\\9p2j5d12gjl5.html:text/html},
}

@misc{noauthor_speech_nodate,
	title = {Speech {Emotion} {Recognition} - {MATLAB} \& {Simulink} - {MathWorks} {United} {Kingdom}},
	url = {https://uk.mathworks.com/help/deeplearning/ug/sequential-feature-selection-for-speech-emotion-recognition.html},
	urldate = {2023-04-25},
	file = {Speech Emotion Recognition - MATLAB & Simulink - MathWorks United Kingdom:C\:\\Users\\kiera\\Zotero\\storage\\VGIITBC4\\sequential-feature-selection-for-speech-emotion-recognition.html:text/html},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	doi = {10.48550/arXiv.1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv:1207.0580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\kiera\\Zotero\\storage\\SLA5VSN4\\Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kiera\\Zotero\\storage\\L5B2FBM7\\1207.html:text/html},
}

@article{quang_does_2021,
	title = {Does {Training} {AI} {Violate} {Copyright} {Law}? {Annual} {Review}},
	volume = {36},
	shorttitle = {Does {Training} {AI} {Violate} {Copyright} {Law}?},
	url = {https://heinonline.org/HOL/P?h=hein.journals/berktech36&i=1466},
	language = {eng},
	number = {4},
	urldate = {2023-05-03},
	journal = {Berkeley Technology Law Journal},
	author = {Quang, Jenny},
	year = {2021},
	pages = {1407--1436},
	file = {Full Text PDF:C\:\\Users\\kiera\\Zotero\\storage\\UBKHGBS3\\Quang - 2021 - Does Training AI Violate Copyright Law Annual Rev.pdf:application/pdf},
}

@misc{loving_current_2023,
	title = {Current {AI} {Copyright} {Cases} – {Part} 1},
	url = {https://copyrightalliance.org/current-ai-copyright-cases-part-1/},
	abstract = {The Unauthorized Use of Copyrighted Material as Training Data    As the world of technology continues to evolve, one of its most intriguing phenomena, artificial intelligence (AI), has taken center stage. While these new technologies offer exciting},
	language = {en-US},
	urldate = {2023-05-03},
	journal = {Copyright Alliance},
	author = {Loving, Tiana},
	month = mar,
	year = {2023},
	file = {Snapshot:C\:\\Users\\kiera\\Zotero\\storage\\89JCUIL6\\current-ai-copyright-cases-part-1.html:text/html},
}
